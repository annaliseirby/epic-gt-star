# Epic GT Star Documentation

Welcome to the Epic GT Star documentation! This document will eventually serve to explain the features of this web app for users, and to document the code for future developers. 

Introduction
------
This project was intended to serve as a one stop shop for cheap, off campus housing for identified Georgia Tech students. Epic GT Star is able to source data from multiple websites, and display off campus housing properties to the users graphically, along with a suggested rent price for the given property. Users are able to request properties between their parameters, such as a price range, a distance from campus, while also comparing with a crime heatmap, to find the cheapest, safest property that suits all of their needs. 

## FAQ
1. Some user question goes here
	Some response

2. Continue this format
	Again, some response

3. At least 3 questions
	Another response

## Technical 
Luckily for this project, there aren't too many different parts. 

### Organization of Code Base
This project has been organized by splitting the code base into the front end and the back end. The back end of this web app has been built with Python, to gather and process data, SQL, to store the processed data, and NodeJS, to create a REST API to connect the front end and the back end. The front end has been built with PHP/ Angular, with a lot of help from external APIs. 

__Project Structure__
+-- BackEnd
|	+-- Database
|	|		_Output Json Files_
|	+-- Programs
|	|	+-- CSVParse.py
|	|	+-- HomeParkRentals.py
|	|	+-- ULoopScraper.py
|	+-- RawCSV
|	|		_CSV from APD_
+FrontEnd
|	+-- HardCoded.js
|	+-- index.html
|	+-- index.js
|	+-- main.css


### Web Scrapers
Currently, there are two webscrapers: one for the [Georgia Tech Uloop Site](https://gatech.uloop.com/ "Gatech Uloop Main"), and one for [Home Park Rentals](https://www.homeparkrentals.com/ "Home Park Rentals Main"). The code for scraping both of these sites is located at `~BackEnd/Programs/`, and are both built using the Python library [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ "Beautiful Soup 4 Documentation"), and [Geopy](https://geopy.readthedocs.io/en/1.10.0/ "Geopy Documentation"), to turn street data into a parsable latitude/ longitude coordinate system.

The Uloop scraper is a bit larger in scope and functionality than the other scraper, as this program is parsing through 20- 30 pages of listings. 


### APD CSV Scraper
The web app has the added functionality of displaying a heat map of crime activity in the region, which is generated by parsing through six months of crime data provided by [Atlanta PD](http://opendata.atlantapd.org/ "Atlanta Police Department Open Database"). Currently, these files are acquired manually, and are placed into the `~BackEnd/RawCSV/` folder.

The scraper goes through all the files placed in this folder, and strips all the information in a row except for the seventh and eighth elements, the latitude and longitude coordinates of the data point, and puts it into a dictionary 

### User Authentication

Next Steps
------
There are still a few issues that need to be addressed in the future. 
* Expand to other sites
* Optimization of web scraper
* Make back end code more robust
* Set up monthly download of CSV file from police
* Set up anonymous chat function
* Create custom Bootstrap CSS file
* Find faster alternative to Geopy